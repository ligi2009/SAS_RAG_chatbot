import streamlit as st
import retriever_chain_openai as rc
import argparse
from langchain.vectorstores import Chroma
# from langchain_community.vectorstores import Chroma
import vectorstore as vs
from retriever_chain_openai import format_docs

# è¨­ç½®å‘½ä»¤è¡Œåƒæ•¸è§£æ
parser = argparse.ArgumentParser(description='Run chatbot with specific vector database.')
parser.add_argument('chemical_number', type=str, help='The SAS chemical number to specify vector database')

args = parser.parse_args()
SAS_chemical_number = args.chemical_number

# è®€å–åŒ–å­¸ç‰©è³ªå°æ‡‰çš„åç¨±
def get_chemical_name(chemical_number, mapping_file='./chemical_mapping.txt'):
    with open(mapping_file, 'r') as file:
        for line in file:
            number, name = line.strip().split(':')
            if number == chemical_number:
                return name
    return "Unknown Chemical"  

# æ ¹æ“šç”¨æˆ¶è¼¸å…¥çš„åŒ–å­¸å“è™Ÿç¢¼ç²å–å°æ‡‰çš„åç¨±
chemical_name = get_chemical_name(SAS_chemical_number)

st.title('ğŸ§ª SAS GPT')
st.caption("ğŸ¦™ A SAS GPT powered by ChatGPT-4o & NeMo-Guardrails") #æ›´æ”¹ä½¿ç”¨æ¨¡å‹åç¨±
st.warning('ğŸ¤– Chatbot with ğŸ§ª  '  + f"{chemical_name}")

with st.sidebar:
    # æ¸…é™¤èŠå¤©æ­·å²æŒ‰éˆ•
    st.button('ğŸ§¹ æ¸…é™¤æŸ¥è©¢è¨˜éŒ„', on_click=lambda: st.session_state.update(messages=[{"role": "assistant", "content": "è«‹è¼¸å…¥åŒ–å­¸ç‰©è³ªç›¸é—œå•é¡Œ"}]))
    st.markdown("[ğŸ”™ å›åˆ°SASå¹³å°](https://sas.cmdm.tw)")

# åˆå§‹åŒ–æœƒè©±ç‹€æ…‹ä¸­çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œå¦‚æœé‚„æ²’æœ‰å‰‡å‰µå»ºä¸€å€‹é»˜èªçš„æ¶ˆæ¯
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "è«‹è¼¸å…¥åŒ–å­¸ç‰©è³ªç›¸é—œå•é¡Œ"}]

# é¡¯ç¤ºæœƒè©±ç‹€æ…‹ä¸­çš„æ‰€æœ‰æ¶ˆæ¯
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# ï¼´ell if it is a summary question 
def is_summary_query(query):
    summary_keywords = ["ç¸½çµ", "æ¦‚è¿°", "æ‘˜è¦", "å›é¡§", "é‡é»", "è¦é»", "æ•´ç†", "summary", "summarize", "summarization", "conclude"]
    return any(keyword in query for keyword in summary_keywords)
# or use NLP model to tell?

def get_response(query):
    try:
        if is_summary_query(query):
            load_path = [f'./Vector_db/59_sum']
        else:
            load_path = [
                f'./Vector_db/59_sum', 
                f'./Vector_db/59_rm_duplicate', 
                f'./Vector_db/59_alternatives_industrial', 
                f'./Vector_db/59_alternatives_children_product', 
                f'./Vector_db/59_alternatives_commercial', 
                f'./Vector_db/59_alternatives_consumer', 
                f'./Vector_db/59_alternatives_consumer_or_commercial', 
                f'./Vector_db/59_alternatives_hydraulic_fluid', 
                f'./Vector_db/59_alternatives_polymers'
            ]
            # load_path = [
            #     f'./Vector_db/59_sum', 
            #     f'./Vector_db/59_rm_duplicate', 
            #     f'./Vector_db/59_1', 
            #     f'./Vector_db/59_2', 
            #     f'./Vector_db/59_3', 
            #     f'./Vector_db/59_4', 
            #     f'./Vector_db/59_5', 
            #     f'./Vector_db/59_6', 
            #     f'./Vector_db/59_7'
            # ]
        # è¨­ç½®RAG Chain é¸ç”¨llm model, embedding model
        chain = rc.chain(load_path=load_path)
        response = chain.invoke(query)
        if isinstance(response, dict):
            response_text = response.get('output', '')
        else:
            response_text = response

        if response_text.strip() == "I'm sorry, I can't respond to that.":
            response_text = "æ­¤å•é¡Œç„¡æ³•å›ç­”ï¼Œè«‹è©¦è‘—è©¢å•å…¶ä»–åŒ–å­¸ç‰©è³ªç›¸é—œå•é¡Œ"

        return response_text, None

    except Exception as e:
        return None, str(e)

# æ¥æ”¶ç”¨æˆ¶è¼¸å…¥çš„æ¶ˆæ¯
if prompt := st.chat_input("è«‹è¼¸å…¥åŒ–å­¸ç‰©è³ªç›¸é—œå•é¡Œ"):

    # å°‡ç”¨æˆ¶æ¶ˆæ¯æ·»åŠ åˆ°æœƒè©±ç‹€æ…‹ä¸­
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    
    # æ§‹å»ºä¸€å€‹æŸ¥è©¢ï¼ŒåªåŒ…å«ç›®å‰ä½¿ç”¨è€…è¼¸å…¥çš„å•é¡Œ
    query = prompt  # åªä½¿ç”¨æœ€æ–°çš„ä½¿ç”¨è€…è¼¸å…¥ä½œç‚ºæŸ¥è©¢

    with st.spinner("Thinking..."):
        response, error = get_response(query)
        
        if error:
            st.error(f"Error: {error}")
        else:
            # å°‡æ¨¡å‹ç”Ÿæˆçš„å›æ‡‰æ·»åŠ åˆ°æœƒè©±ç‹€æ…‹ä¸­ä¸¦é¡¯ç¤º
            st.session_state.messages.append({"role": "assistant", "content": response})
            st.chat_message("assistant").write(response)

# æ¸…é™¤èŠå¤©æ­·å²åŠŸèƒ½å’ŒæŒ‰éˆ•
def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "è«‹è¼¸å…¥åŒ–å­¸ç‰©è³ªç›¸é—œå•é¡Œ"}]
